# SmartPDF: Connecting the Dots - Round 1A: Understand Your Document

## ğŸ’¡ Rethink Reading. Rediscover Knowledge.

This repository contains the solution for **Round 1A: Understand Your Document** of the Adobe India Hackathon's "Connecting the Dots" challenge. Our mission is to transform static PDFs into intelligent, structured outlines, making them machine-readable and paving the way for smarter document experiences.

---

## ğŸš€ Features

* **PDF Parsing Powerhouse:** Leverages `PyMuPDF` for robust and efficient extraction of every text span, including content, precise font sizes, positional data, and crucial font flags (bold, italic).
* **Intelligent Outline Extraction:** Employs a custom-trained `RandomForestClassifier` to accurately identify document titles and hierarchical headings (H1, H2, H3).
* **Seamless Aggregation:** Implements a sophisticated post-processing step to combine continuous text spans belonging to the same predicted heading level (or title) on the same page. This ensures a clean, cohesive, and highly readable output outline.
* **Multilingual Support:** Automatically detects the language of the document title and each extracted heading using the `langdetect` library, adding valuable linguistic metadata for global document understanding.
* **Offline First:** Designed for complete offline execution, ensuring high performance and adherence to hackathon constraints.
* **Dockerized for Portability:** Comes with a meticulously crafted `Dockerfile` to provide a consistent, reproducible, and self-contained environment for effortless building and deployment.

---

## ğŸ› ï¸ Tech Stack Used

* **Python 3.10:** The backbone of our solution.
* **PyMuPDF (fitz):** For lightning-fast PDF text and metadata extraction.
* **scikit-learn:** Our choice for robust machine learning model training and inference.
* **NumPy & Pandas:** Essential for efficient numerical computations and data manipulation.
* **Joblib:** For streamlined serialization and deserialization of our trained models.
* **Click:** Powers our user-friendly command-line interface.
* **langdetect:** Our go-to for accurate multilingual language identification.
* **Docker:** For seamless containerization and environment consistency.

---

## ğŸ“‚ Project Structure

Our modular project design keeps Round 1A self-contained for clarity and ease of use.





Challenge_1A/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input_pdfs/               # ğŸ“¥ Your PDFs for processing/training go here
â”‚   â”œâ”€â”€ annotations_template.csv  # ğŸ“ Generated template for manual labeling (Step 1 of training)
â”‚   â””â”€â”€ annotations.csv           # ğŸ“Š Your custom, manually labeled training dataset for R1A model
â”œâ”€â”€ models/
â”‚   â””â”€â”€ heading_model.joblib      # ğŸ§  Trained R1A heading detection model (Output of training)
â”œâ”€â”€ output/                       # ğŸ“„ Directory for generated JSON outlines
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ export_spans.py           # ğŸš€ Script to extract raw spans from PDFs (for annotations_template.csv)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ init.py               # Python package marker
â”‚   â”œâ”€â”€ main.py                   # ğŸ Main entry point for running R1A (training or inference)
â”‚   â”œâ”€â”€ round1a/                  # Core logic for Round 1A
â”‚   â”‚   â”œâ”€â”€ init.py           # Python package marker
â”‚   â”‚   â”œâ”€â”€ feature_extractor.py  # Extracts numerical features from PDF spans
â”‚   â”‚   â”œâ”€â”€ heading_detector.py   # Loads model, predicts, aggregates, and adds language detection
â”‚   â”‚   â””â”€â”€ pdf_parser.py         # Parses PDF content into text spans
â”‚   â””â”€â”€ train_model.py            # ğŸš‚ R1A model training script
â”œâ”€â”€ venv/                         # Python virtual environment (recommended for local development)
â”œâ”€â”€ Dockerfile                    # ğŸ³ Dockerfile for building the R1A solution container
â””â”€â”€ requirements.txt              # Python dependencies for R1A




---

## ğŸ“ˆ Flow Diagram and Working Explanation

### Flow Diagram

```mermaid
graph TD
    A[Start] --> B{PDF Document};
    B --> C[PDF Parser (pdf_parser.py)];
    C --> D{Text Spans + Metadata};
    D --> E[Feature Extractor (feature_extractor.py)];
    E --> F{Numerical Features};
    F --> G[Trained Model (heading_model.joblib)];
    G --> H{Predicted Labels (Title, H1, H2, H3, Body)};
    H --> I[Heading Detector (heading_detector.py)];
    I --> J{Aggregation Logic};
    J --> K{Language Detector (langdetect)};
    K --> L{Combined Headings + Language};
    L --> M[Output JSON];
    M --> N[End];

    subgraph Training Flow
        O[Start Training] --> P{Input PDFs};
        P --> Q[Export Spans (export_spans.py)];
        Q --> R{Raw Spans CSV (annotations_template.csv)};
        R --> S[Manual Labeling];
        S --> T{Labeled Dataset (annotations.csv)};
        T --> U[Train Model (train_model.py)];
        U --> V{Trained Model (heading_model.joblib)};
        V --> W[End Training];
    end